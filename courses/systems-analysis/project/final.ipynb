{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a Project of a Chatbot based on LLM\n",
    "\n",
    "This is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirements\n",
    "\n",
    "As follows the required dependencies will be instaled into workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers faiss-cpu datasets fastapi uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract PDF Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "path = \"docs/info.pdf\"\n",
    "text = extract_text_from_pdf(path)\n",
    "print(text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Model based on a Foundational Model and Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# load foundational model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# prepare specific data\n",
    "path_concepts = \"data/concepts.pdf\"\n",
    "text_concepts = extract_text_from_pdf(path_concepts)\n",
    "dataset = Dataset.from_dict({\"text\": [text_concepts]})\n",
    "\n",
    "# tokenize data\n",
    "def tokenize_function(concepts):\n",
    "    return tokenizer(concepts[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# train model with fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# load retriever\n",
    "index = faiss.IndexFlatL2(768)\n",
    "documents = [\"data/updates.pdf\"]\n",
    "embeddings = np.array([embed(doc) for doc in documents])\n",
    "index.add(embeddings)\n",
    "\n",
    "def retrieve(query: str) -> str:\n",
    "    query_embedding = embed(query)\n",
    "    D, I = index.search(np.array([query_embedding]), 1)\n",
    "    return [documents[i] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load generator\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate_response(prompt: str) -> str:\n",
    "    retrieved_document = retrieve(prompt)\n",
    "    context = \" \".join(retrieved_document)\n",
    "    input_text = f\"{context} {prompt}\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    output = model.generate(inputs, max_length=150, num_return_sequences=1)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# example\n",
    "prompt = \"What is the latest update?\"\n",
    "response = generate_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deploy FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "@app.post(\"/chatbot\")\n",
    "def chatbot(query: Query):\n",
    "    try:\n",
    "        response = generate_response(query.prompt)\n",
    "        return {\"response\": response}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)               \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
