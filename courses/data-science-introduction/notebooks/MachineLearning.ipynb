{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "__Machine learning__ is a method of _data analysis_ that automates\n",
    "_analytical model building_.\n",
    "It is a branch of _artificial intelligence_ based on the idea that systems\n",
    "can _learn from data_, _identify patterns_ and _make decisions_ with\n",
    "minimal human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn (SKlearn, Scipy, Numpy)\n",
    "\n",
    "__Scikit-learn__ is a _Python package_ that provides a wide range of _machine learning algorithms_ and tools. \n",
    "It is built on top of _NumPy_, _SciPy_, and _Matplotlib_, and is designed to be simple and efficient for data analysis and modeling.\n",
    "\n",
    "__Scikit-learn__ offers various modules for tasks such as _classification_, _regression_, _clustering_, _dimensionality reduction_, and _model selection_.\n",
    "It also provides utilities for _preprocessing data_, _evaluating models_, and _handling datasets_.\n",
    "\n",
    "With its extensive documentation and user-friendly interface, __Scikit-learn__ is widely used in the field of machine learning and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spplitting the data into training and testing data\n",
    "\n",
    "# importing the model\n",
    "\n",
    "# Assuming football_df is your DataFrame and it has a 'target' column\n",
    "# columns_list is a list of column names to be used as features\n",
    "\n",
    "# Splitting the data into features and target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "__K-Nearest Neighbors__ is a simple algorithm that _stores all available\n",
    "cases_ and _classifies_ new cases based on a similarity measure.\n",
    "\n",
    "It is a type of _instance-based learning_, or _lazy learning_, where the\n",
    "function is only approximated locally and all computation is deferred\n",
    "until function evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of the data\n",
    "\n",
    "# Feature scaling for better performance of KNN\n",
    "\n",
    "# Creating the KNN model\n",
    "\n",
    "# Fitting the model with the training data\n",
    "\n",
    "# Testing model performance\n",
    "\n",
    "# Predicting the class for the new input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression of the data\n",
    "\n",
    "# Feature scaling for better performance of KNN\n",
    "\n",
    "# Creating the KNN model for regression\n",
    "\n",
    "# Fitting the model with the training data\n",
    "\n",
    "# Assuming new_input is a new data point you want to predict\n",
    "# new_input should be a list of values corresponding to columns_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Least Squares\n",
    "\n",
    "__Linear regression__ is a type of _regression analysis_ used for predicting the value of a _continuous dependent variable_. It works by finding the _line that best fits the data_.\n",
    "\n",
    "_Least squares_ is a method for finding the _best-fitting_ line by __minimizing__ the _sum of the squared differences_ between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Linear Regression model\n",
    "\n",
    "# Fitting the model with the training data\n",
    "\n",
    "# Assuming new_input is a new data point you want to predict\n",
    "# new_input should be a list of values corresponding to columns_list\n",
    "\n",
    "# Predicting the target for the new input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization with Ridge and Lasso\n",
    "\n",
    "__Ridge regression__ (_L2_) and __Lasso regression__ (_L1_) are a type of _linear regression_ that includes a _penalty_ term to __prevent overfitting__. They work by adding a _regularization term_ to the least squares objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing Rigde Regression (L2 regularization)\n",
    "\n",
    "# Creating the Ridge Regression model\n",
    "# alpha is the regularization strength; larger values specify stronger regularization.\n",
    "\n",
    "# Fitting the model with the training data\n",
    "\n",
    "# Making predictions on the test set\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing Lasso Regression (L1 regularization)\n",
    "# Creating the Lasso Regression model\n",
    "# alpha is the regularization strength; larger values specify stronger regularization.\n",
    "\n",
    "# Fitting the model with the training data\n",
    "\n",
    "# Making predictions on the test set\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n",
    "\n",
    "# To understand feature sensitivity, you can look at the coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "__Polynomial regression__ is a type of r_egression analysis_ that models\n",
    "the _relationship_ between the independent and dependent variables as\n",
    "an $nth-degree$ _polynomial_. It can capture _non-linear relationships_ between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming football_df is your DataFrame and it has a 'target' column\n",
    "# columns_list is a list of column names to be used as features\n",
    "\n",
    "# Splitting the data into features and target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "\n",
    "# Transforming the features into polynomial features\n",
    "\n",
    "# Creating the Linear Regression model\n",
    "\n",
    "# Fitting the model with the polynomial features and the training data\n",
    "\n",
    "# Making predictions on the test set\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "__Logistic regression__ is a type of _regression analysis_ used for predicting the outcome of a _categorical dependent variable_.\n",
    "It is used for __binary classification__ tasks, where the output is a\n",
    "probability between $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the Logistic Regression model\n",
    "\n",
    "# Fitting the model with the training data\n",
    "\n",
    "# Making predictions on the test set\n",
    "\n",
    "# Calculating the accuracy of the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "__Cross-validation__ is a technique for _assessing the performance_ of a\n",
    "model. It involves _splitting_ the data into multiple subsets, training the model on some subsets, and evaluating it on others.\n",
    "\n",
    "__Cross-validation__ helps to _reduce overfitting_ and provides a more\n",
    "accurate estimate of the modelâ€™s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a logistic regression model\n",
    "\n",
    "# Perform cross-validation\n",
    "\n",
    "# Print the accuracy for each fold\n",
    "\n",
    "# Print the mean accuracy of all 5 folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "__One-hot encoding__ is a technique for _converting_ _categorical_ variables into _numerical_ variables.\n",
    "\n",
    "It creates a _binary vector_ for each _category_, with a $1$ for the\n",
    "category and $0$s for all other categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a fake DataFrame\n",
    "\n",
    "# Display the original DataFrame\n",
    "\n",
    "# Applying OneHotEncoder\n",
    "\n",
    "# Creating a DataFrame with the encoded data\n",
    "\n",
    "# Concatenating the encoded columns with the original DataFrame (excluding the original 'Color' and 'Size' columns)\n",
    "\n",
    "# Display the final DataFrame after one-hot encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "__Random forest__ is an _ensemble learning_ method that combines\n",
    "_multiple decision trees_ to create a strong predictive model.\n",
    "\n",
    "It works by building _multiple trees_ and averaging their predictions to\n",
    "_reduce overfitting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Trees\n",
    "\n",
    "__Gradient boosted decision trees__ are an _ensemble learning_ method\n",
    "that combines _multiple decision trees_ and _gradient descedent\n",
    "optimization_ to create a strong predictive model.\n",
    "\n",
    "They work by building _trees sequentially_, with each tree _correcting the\n",
    "errors_ of the previous trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "__Neural networks__ are a type of _machine learning_ model inspired by\n",
    "the _human brain_.\n",
    "\n",
    "They consist of _layers of interconnected nodes_ that process input data\n",
    "and produce output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "A __confusion matrix__ is a table that _summarizes the performance_ of a\n",
    "classification model.\n",
    "\n",
    "It shows the number of _true positives_, _true negatives_, _false positives_,\n",
    "and _false negatives_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Metrics\n",
    "\n",
    "- __Accuracy:__ The proportion of correct predictions.\n",
    "- __Precision:__ The proportion of true positives among all positive\n",
    "predictions.\n",
    "- __Recall:__ The proportion of true positives among all actual positives.\n",
    "- __F1 Score:__ The harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Decision Metrics\n",
    "\n",
    "- __ROC Curve:__ A plot of the true positive rate against the false positive rate.\n",
    "- __Precision-Recall Curve:__ A plot of precision against recall.\n",
    "- __AUC-ROC:__ The area under the ROC curve.\n",
    "- __AUC-PR:__ The area under the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Evaluation Metrics\n",
    "\n",
    "- __Mean Squared Error:__ The average of the squared differences between the predicted and actual values.\n",
    "- __Mean Absolute Error:__ The average of the absolute differences between the predicted and actual values.\n",
    "- __R-Squared:__ The proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "- __Adjusted R-Squared:__ A modified version of R-squared that adjusts for the number of predictors in the model.\n",
    "- __Root Mean Squared Error:__ The square root of the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udistrital",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
