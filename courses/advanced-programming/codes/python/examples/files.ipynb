{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files Management with Python\n",
    "\n",
    "This is a _notebook_ with some simple codes to __handle__ files.\n",
    "\n",
    "Files are a simple way to handle data persistance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use plain text files\n",
    "\n",
    "There are _three types_ of access for a file, depending of the need: __r__ (_read_), __w__ (_write_), and __a__ (_append_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to write in a file\n",
    "file_name = \"names.txt\"\n",
    "with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    text = \"Alice Bob Claire Denisse\"\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use append to file to avoid overwriting\n",
    "with open(file_name, \"a\", encoding=\"utf-8\") as f:\n",
    "    text = \"\\nErwin Fidelia Giselle\"\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use r mode to read from file\n",
    "# it will read the full text\n",
    "with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    print(\"Full Text:\", text)\n",
    "\n",
    "print(\"=\"*20)\n",
    "# it will read the text line by line\n",
    "with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        print(\"Line:\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV (Comma-Separated Values) Files\n",
    "\n",
    "A __CSV__ file is a format where each _jump of line_ is a new __row__. Into each row, the values are _separated by comma_ (or a specific symbol), as a __column__. Traditionally, it _replaces_ a __relational database__ for small amount of data for just one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv files -> comman separate value\n",
    "\n",
    "csv_file = \"personas.csv\"\n",
    "text = \"name,code,email\\nalice,1,a@ud.co\\nbob,2,b@ud.co\\nclaire,3,c@ud.co\"\n",
    "\n",
    "with open(csv_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv and put into a dictionary\n",
    "with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    personas = f.readlines()\n",
    "\n",
    "columns = personas[0].replace(\"\\n\", \"\").split(\",\")\n",
    "personas_data = personas[1:] # slicing to remove columns names\n",
    "\n",
    "final_data = []\n",
    "for person in personas_data:\n",
    "    person_data = {}\n",
    "    person = person.replace(\"\\n\", \"\")\n",
    "    temp_data = person.split(\",\")\n",
    "    for i, value in enumerate(temp_data):\n",
    "        person_data[columns[i]] = value\n",
    "    final_data.append(person_data)\n",
    "\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the read data, print the name and email based on keys of the dictionary\n",
    "for person in final_data:\n",
    "    print(person[\"name\"], person[\"email\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Files\n",
    "\n",
    "A _JSON file_ is a format of file where a _data structure_ called __key:value__ is used to persist data.\n",
    "It is the equivalent to a __dictionary__ or a __list of dictionaries__ in _python_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# write a json file based on the data of the csv file\n",
    "\n",
    "json_file = \"personas.json\"\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from json and load into a list of dictionaries\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dummy\n",
    "\n",
    "__Data Dummy__ is a dataset with _fake data_ just to _test_ the application. Here the idea is to__ validate__ __codes__, __workflows__, __outputs__, but _without expose real data_ or avoid dependencies of client's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use FAKER python package, it is used to generate fake data\n",
    "!pip install faker\n",
    "import faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fake_data = faker.Faker()\n",
    "students = []\n",
    "n = 1000000\n",
    "\n",
    "# generate a dataset of fake n students and save it into a json file\n",
    "def generate_fake_students(file_name, n):\n",
    "    students = []\n",
    "    for i in range(n):\n",
    "        student = {}\n",
    "        student[\"name\"] = fake_data.name()\n",
    "        student[\"lastname\"] = fake_data.last_name()\n",
    "        student[\"email\"] = fake_data.email()\n",
    "        student[\"address\"] = fake_data.address()\n",
    "        student[\"phone\"] = fake_data.phone_number()\n",
    "        student[\"bitrhday\"] = str(fake_data.date_of_birth())\n",
    "        student[\"country\"] = fake_data.country()\n",
    "        students.append(student)\n",
    "\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(students, f)\n",
    "\n",
    "generate_fake_students(\"students_ind.json\", n)\n",
    "generate_fake_students(\"students_sist.json\", n)\n",
    "generate_fake_students(\"students_elec.json\", n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Access to Data\n",
    "\n",
    "Here the idea is to check the time an algorithm to count the time to get the repetitions per name including the load process from the three files, but one by one as a lineal process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_names(data, names):\n",
    "    for student in data:\n",
    "        name = student[\"name\"]\n",
    "        if name in names:\n",
    "            names[name] += 1\n",
    "        else:\n",
    "            names[name] = 1\n",
    "    return names\n",
    "\n",
    "names = {}\n",
    "\n",
    "with open(\"students_elec.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_elec = json.load(f)\n",
    "    names = count_names(data_elec, names)\n",
    "\n",
    "with open(\"students_sist.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_sist = json.load(f)\n",
    "    names = count_names(data_sist, names)\n",
    "\n",
    "with open(\"students_ind.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_ind = json.load(f)\n",
    "    names = count_names(data_ind, names)\n",
    "\n",
    "final_students = data_elec + data_sist + data_ind\n",
    "print(len(final_students))\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel access to Files\n",
    "\n",
    "Here the idea is to use threads to load the files and count on each name ther repetitions by name. After, a merge of the results is make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def read_file(file_name):\n",
    "    names = {}\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for student in data:\n",
    "        name = student[\"name\"]\n",
    "        if name in names:\n",
    "            names[name] += 1\n",
    "        else:\n",
    "            names[name] = 1\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_names(names_lists):\n",
    "    final_names = {}\n",
    "    for names in names_lists:\n",
    "        for name in names:\n",
    "            if name in final_names:\n",
    "                final_names[name] += names[name]\n",
    "            else:\n",
    "                final_names[name] = names[name]\n",
    "    return final_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_analysis():\n",
    "    list_files = [\"students_elec.json\", \n",
    "                  \"students_sist.json\", \n",
    "                  \"students_ind.json\"]\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        lists_results = list(executor.map(read_file, list_files))\n",
    "\n",
    "    final_names = merge_names(lists_results)\n",
    "    print(len(final_names))\n",
    "\n",
    "parallel_analysis()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "__Serialization__ is the process to take an object, _convert to binary_, and it is available to save as an encoded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "with open(\"students_elec.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_elec = json.load(f)\n",
    "\n",
    "with open(\"students_elec.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_elec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"students_elec.pkl\", \"rb\") as f:\n",
    "    data_elec = pickle.load(f)\n",
    "print(data_elec[:6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
