{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5146cd0c",
   "metadata": {},
   "source": [
    "# Neural Networks and Reinforcement Learning: Advanced Architectures and Applications\n",
    "\n",
    "This notebook demonstrates advanced neural network architectures for sequence modeling (RNN, LSTM, NARX) using PyTorch and real-world Kaggle datasets, as well as reinforcement learning algorithms (DQN, Policy Gradient, Actor-Critic) using stable-baselines3 and OpenAI Gym.\n",
    "\n",
    "**Author:** Carlos Andrés Sierra, M.Sc.\n",
    "\n",
    "**Course:** Systems Sciences Foundations\n",
    "\n",
    "**Universidad Distrital Francisco José de Caldas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a17c05",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Requirements](#setup)\n",
    "2. [Neural Networks](#nn)\n",
    "    - [Recurrent Neural Network (RNN)](#rnn)\n",
    "    - [Long Short-Term Memory (LSTM)](#lstm)\n",
    "    - [Nonlinear Autoregressive Networks (NARX)](#narx)\n",
    "3. [Reinforcement Learning](#rl)\n",
    "    - [Deep Q-Networks (DQN)](#dqn)\n",
    "    - [Policy Gradient Methods](#pg)\n",
    "    - [Actor-Critic Methods](#ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a152b",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Setup and Requirements\n",
    "\n",
    "Install and import the required libraries for neural networks, data handling, and reinforcement learning. This includes PyTorch, pandas, matplotlib, kaggle, and stable-baselines3. You will also need to set up the Kaggle API for dataset downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install scikit-learn torch torchtext pandas matplotlib  stable-baselines3[extra] gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13910d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# For RL\n",
    "import gym\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2d369",
   "metadata": {},
   "source": [
    "<a id=\"nn\"></a>\n",
    "# Neural Networks\n",
    "\n",
    "This section covers advanced neural network architectures for sequence modeling using PyTorch and real-world datasets from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d4623",
   "metadata": {},
   "source": [
    "<a id=\"rnn\"></a>\n",
    "## Recurrent Neural Network (RNN)\n",
    "\n",
    "We will use the [Household Electric Power Consumption Dataset](https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set) from Kaggle. The goal is to predict future power consumption using a simple RNN.\n",
    "\n",
    "**Steps:**\n",
    "1. Download and preprocess the dataset.\n",
    "2. Build a PyTorch RNN for sequence prediction.\n",
    "3. Train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd090c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> tuple:\n",
    "    \"\"\"Load and preprocess household power consumption data.\n",
    "    \n",
    "    \n",
    "    This function reads the data from a CSV file, processes it, and returns a DataFrame\n",
    "    with scaled numerical values and datetime as the index.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the scaled household power consumption data and the scaler used for scaling.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('household_power_consumption.txt', sep=';', na_values=['?'])\n",
    "\n",
    "    # Combine date and time, drop missing values\n",
    "    df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "    df = df.drop(['Date', 'Time'], axis=1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    df[num_cols] = df[num_cols].astype(float)\n",
    "\n",
    "    # Scale all numerical columns\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "    # Set datetime as index (optional)\n",
    "    return df_scaled.set_index('datetime'), scaler\n",
    "\n",
    "# Convert all relevant columns to float\n",
    "num_cols = ['Global_active_power', 'Global_reactive_power', 'Voltage', \n",
    "            'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "    \n",
    "df_scaled, scaler = load_data()\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "\n",
    "def create_sequences(data: np.ndarray, seq_length: int) -> tuple:\n",
    "    \"\"\"\n",
    "    This function createa sequences of data for RNN input.\n",
    "    It takes a numpy array and a sequence length as input and returns\n",
    "    a tuple of sequences and targets.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Input data.\n",
    "        seq_length (int): Length of each sequence.\n",
    "    Returns:\n",
    "        tuple: Tuple of (sequences, targets).\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[i+seq_length, 0]  # Predict only the first column (target)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Function to split data into train and test sets by percentage\n",
    "def train_test_split_sequences(X: np.ndarray, y: np.ndarray, train_pct: float = 0.8) -> tuple:\n",
    "    \"\"\"\n",
    "    This function aplita the data into training and testing sets based on the given percentage.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input features.\n",
    "        y (np.ndarray): Target values.\n",
    "        train_pct (float): Percentage of data to use for training.\n",
    "    Returns:\n",
    "        tuple: Tuple of (X_train, y_train, X_test, y_test).\n",
    "    \"\"\"\n",
    "    n_train = int(len(X) * train_pct)\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_test, y_test = X[n_train:], y[n_train:]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple RNN model and train on the scaled dataframe\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int = 32, num_layers: int = 1):\n",
    "        \"\"\"\n",
    "        Initializes the SimpleRNN model.\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            hidden_size (int): Number of features in the hidden state.\n",
    "            num_layers (int): Number of recurrent layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN and fully connected layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_size).\n",
    "        Returns:\n",
    "            Output tensor after passing through the RNN and fully connected layer.\n",
    "        \"\"\"\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out).squeeze()\n",
    "\n",
    "\n",
    "def train_rnn(model: nn.Module, X_train_t: torch.Tensor, y_train_t: torch.Tensor, epochs: int = 20) -> list:\n",
    "    \"\"\"\n",
    "    Train the RNN model on the training data.\n",
    "\n",
    "    Args:\n",
    "        model: The RNN model to train.\n",
    "        X_train_t (torch.Tensor): Training input data.\n",
    "        y_train_t (torch.Tensor): Training target data.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        List of training losses for each epoch.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_t)\n",
    "        loss = criterion(output, y_train_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "seq_length = 24\n",
    "data = df_scaled[num_cols].values[:50000]  # Use all features\n",
    "X, y = create_sequences(data, seq_length)\n",
    "X_train, y_train, X_test, y_test = train_test_split_sequences(X, y, train_pct=0.8)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleRNN(input_size=len(num_cols))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "losses = train_rnn(model, X_train_t, y_train_t, epochs=50)\n",
    "\n",
    "# Plot training loss\n",
    "epochs = np.arange(1, len(losses) + 1)\n",
    "plt.plot(epochs, losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('RNN Training Loss')\n",
    "plt.xticks(epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set and plot results with x axis labels starting from 1 (no zero)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(X_test_t).numpy()\n",
    "    true = y_test_t.numpy()\n",
    "    # Inverse transform only the target column using the same scaler and num_cols\n",
    "    pred_inv = scaler.inverse_transform(\n",
    "        np.concatenate([pred.reshape(-1, 1), np.zeros((pred.shape[0], len(num_cols)-1))], axis=1)\n",
    "    )[:, 0]\n",
    "    true_inv = scaler.inverse_transform(\n",
    "        np.concatenate([true.reshape(-1, 1), np.zeros((true.shape[0], len(num_cols)-1))], axis=1)\n",
    "    )[:, 0]\n",
    "\n",
    "# Set x axis to start at 1 and increment by 1 for better visualization\n",
    "x = np.arange(1, len(true_inv) + 1)\n",
    "plt.plot(x, true_inv, label='True')\n",
    "plt.plot(x, pred_inv, label='Predicted')\n",
    "plt.title('RNN: Power Consumption Prediction')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Global Active Power (kW)')\n",
    "plt.xticks(x[::500])  # Show every time step as a tick, starting from 1\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d870c15",
   "metadata": {},
   "source": [
    "<a id=\"lstm\"></a>\n",
    "## Long Short-Term Memory (LSTM)\n",
    "\n",
    "We will use the [Air Passengers Dataset](https://www.kaggle.com/datasets/rakannimer/air-passengers) from Kaggle. The goal is to forecast stock prices using an LSTM.\n",
    "\n",
    "**Steps:**\n",
    "1. Download and preprocess the dataset.\n",
    "2. Build a PyTorch LSTM for sequence forecasting.\n",
    "3. Train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8baccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load stock dataset\n",
    "file = \"AirPassengers.csv\"\n",
    "    \n",
    "if not os.path.exists(file):\n",
    "    print(\"Not dataset found.\")\n",
    "    \n",
    "def load_air_passengers():\n",
    "    \"\"\"\n",
    "    Load and preprocess the Air Passengers dataset.\n",
    "    Returns the scaled dataframe, scaler, and original dataframe.\n",
    "    \"\"\"\n",
    "    passenger_df = pd.read_csv(file, parse_dates=['Month'])\n",
    "    passenger_df = passenger_df.set_index('Month')\n",
    "    passenger_df = passenger_df.rename(columns={'#Passengers': 'Passengers'})\n",
    "    \n",
    "    # Add lagged features and rolling mean as extra columns\n",
    "    passenger_df['lag1'] = passenger_df['Passengers'].shift(1)\n",
    "    passenger_df['lag2'] = passenger_df['Passengers'].shift(2)\n",
    "    passenger_df['rolling_mean_3'] = passenger_df['Passengers'].rolling(window=3).mean()\n",
    "    passenger_df = passenger_df.dropna()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    feature_cols = ['Passengers', 'lag1', 'lag2', 'rolling_mean_3']\n",
    "    passenger_df[feature_cols] = scaler.fit_transform(passenger_df[feature_cols])\n",
    "    return passenger_df, scaler, feature_cols\n",
    "\n",
    "def create_sequences(data: np.ndarray, seq_length: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Create sequences for multivariate time series data.\n",
    "    Args:\n",
    "        data (np.ndarray): Input data.\n",
    "        seq_length (int): Length of each sequence.\n",
    "    Returns:\n",
    "        Tuple of (sequences, targets).\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i+seq_length), :]\n",
    "        y = data[i+seq_length, 0]  # Predict only the original Passengers column\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def train_test_split_sequences(X: np.ndarray, y: np.ndarray, train_pct: float = 0.8) -> tuple:\n",
    "    \"\"\"\n",
    "    Split sequences into train and test sets.\n",
    "    Args:\n",
    "        X (np.ndarray): Input sequences.\n",
    "        y (np.ndarray): Target values.\n",
    "        train_pct (float): Percentage of data for training.\n",
    "    Returns:\n",
    "        Tuple of (X_train, y_train, X_test, y_test).\n",
    "    \"\"\"\n",
    "    n_train = int(len(X) * train_pct)\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_test, y_test = X[n_train:], y[n_train:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Load and preprocess data\n",
    "passengers_df, scaler, feature_cols = load_air_passengers()\n",
    "data = passengers_df[feature_cols].values\n",
    "seq_length = 12  # Use past 12 months to predict next month\n",
    "X, y = create_sequences(data, seq_length)\n",
    "X_train, y_train, X_test, y_test = train_test_split_sequences(X, y, train_pct=0.7)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"This class defines a simple LSTM model for time series prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int = 1, hidden_size: int = 32, num_layers: int = 1):\n",
    "        \"\"\"\n",
    "        Initializes the SimpleLSTM model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            hidden_size (int): Number of features in the hidden state.\n",
    "            num_layers (int): Number of recurrent layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM and fully connected layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_size).\n",
    "        Returns:\n",
    "            Output tensor after passing through the LSTM and fully connected layer.\n",
    "        \"\"\"\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out).squeeze()\n",
    "\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleLSTM(input_size=len(feature_cols))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_lstm(model: nn.Module, X_train_t: torch.Tensor, y_train_t: torch.Tensor, epochs: int = 20) -> list:\n",
    "    \"\"\"\n",
    "    Train the LSTM model and print loss every 5 epochs.\n",
    "\n",
    "    Args:\n",
    "        model: The LSTM model to train.\n",
    "        X_train_t (torch.Tensor): Training input data.\n",
    "        y_train_t (torch.Tensor): Training target data.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        List of training losses for each epoch.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_t)\n",
    "        loss = criterion(output, y_train_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    return losses\n",
    "\n",
    "losses = train_lstm(model, X_train_t, y_train_t, epochs=50)\n",
    "\n",
    "\n",
    "# Plot training loss\n",
    "epochs_arr = np.arange(1, len(losses) + 1)\n",
    "plt.plot(epochs_arr, losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('LSTM Training Loss')\n",
    "plt.xticks(epochs_arr)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf622e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set and plot predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(X_test_t).numpy()\n",
    "    true = y_test_t.numpy()\n",
    "    # Inverse transform only the target column\n",
    "    pred_inv = scaler.inverse_transform(\n",
    "        np.concatenate([pred.reshape(-1, 1), np.zeros((pred.shape[0], len(feature_cols)-1))], axis=1)\n",
    "    )[:, 0]\n",
    "    true_inv = scaler.inverse_transform(\n",
    "        np.concatenate([true.reshape(-1, 1), np.zeros((true.shape[0], len(feature_cols)-1))], axis=1)\n",
    "    )[:, 0]\n",
    "\n",
    "x = np.arange(1, len(true_inv) + 1)\n",
    "plt.plot(x, true_inv, label='True')\n",
    "plt.plot(x, pred_inv, label='Predicted')\n",
    "plt.title('LSTM: Air Passengers Prediction (Multivariate)')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a14dd",
   "metadata": {},
   "source": [
    "<a id=\"narx\"></a>\n",
    "## Nonlinear Autoregressive Networks (NARX)\n",
    "\n",
    "We will use the [Daily Minimum Temperatures in Melbourne](https://www.kaggle.com/datasets/paulbrabban/daily-minimum-temperatures-in-melbourne) dataset. We'll implement a NARX-like architecture in PyTorch for multi-step forecasting.\n",
    "\n",
    "**Steps:**\n",
    "1. Download and preprocess the dataset.\n",
    "2. Implement a NARX-like model (feedforward with lagged inputs).\n",
    "3. Demonstrate multi-step forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14790778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load temperature dataset \n",
    "file = \"daily-minimum-temperatures-in-me.csv\"\n",
    "\n",
    "if not os.path.exists(file):\n",
    "    print(\"Not dataset found.\")\n",
    "\n",
    "def load_temperature_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Load and preprocess the daily minimum temperatures dataset.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the scaled temperature data and the scaler used for scaling.\n",
    "    \"\"\"\n",
    "    df_temp = pd.read_csv(file, parse_dates=['Date'])\n",
    "    df_temp = df_temp.sort_values('Date')\n",
    "    df_temp = df_temp.set_index('Date')\n",
    "    df_temp = df_temp.rename(columns={'Daily minimum temperatures in Melbourne, Australia, 1981-1990': 'Temperature'})\n",
    "    \n",
    "    df_temp = df_temp[~df_temp['Temperature'].astype(str).str.contains(r'\\?', regex=True)].dropna()\n",
    "    # Convert to float and fill missing values with the mean\n",
    "    df_temp['Temperature'] = df_temp['Temperature'].astype(float) \n",
    "    df_temp['Temperature'] = df_temp['Temperature'].fillna(df_temp['Temperature'].mean())\n",
    "    \n",
    "    # Scale the temperature column\n",
    "    scaler_temp = MinMaxScaler()\n",
    "    df_temp['temp'] = scaler_temp.fit_transform(df_temp[['Temperature']])\n",
    "    \n",
    "    df_temp = df_temp[['temp']]\n",
    "    print(df_temp.head())\n",
    "    return df_temp, scaler_temp\n",
    "\n",
    "\n",
    "def create_narx_sequences(data: np.ndarray, input_lags=10, output_lags=5) -> tuple:\n",
    "    \"\"\"\n",
    "    Create NARX sequences for time series data.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Input data.\n",
    "        input_lags (int): Number of input lags.\n",
    "        output_lags (int): Number of output lags.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (input sequences, output sequences).\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - input_lags - output_lags):\n",
    "        x = data[i:(i+input_lags)]\n",
    "        y = data[(i+input_lags):(i+input_lags+output_lags)]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "input_lags = 10\n",
    "output_lags = 5\n",
    "df_temp, scaler_temp = load_temperature_data()\n",
    "data = df_temp['temp'].values\n",
    "X, y = create_narx_sequences(data, input_lags, output_lags)\n",
    "\n",
    "X_train, y_train = X[:2750], y[:2750]\n",
    "X_test, y_test = X[2750:], y[2750:]\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686162ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NARXNet(nn.Module):\n",
    "    \"\"\"This class implements a NARX-like model using a multi-layer perceptron (MLP). \"\"\"\n",
    "\n",
    "    def __init__(self, input_lags: int, output_lags: int, hidden_size: int = 32):\n",
    "        \"\"\"\n",
    "        Initializes the NARXNet model.\n",
    "        \n",
    "        Args:\n",
    "            input_lags (int): Number of input lags.\n",
    "            output_lags (int): Number of output lags.\n",
    "            hidden_size (int): Number of features in the hidden layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_lags, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_lags)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_lags).\n",
    "\n",
    "        Returns:\n",
    "            Output tensor after passing through the MLP.\n",
    "        \"\"\"\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "narx_model = NARXNet(input_lags, output_lags)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(narx_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(50):\n",
    "    narx_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = narx_model(X_train_t)\n",
    "    loss = criterion(output, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('NARX Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04989813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-step forecasting on test set\n",
    "narx_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = narx_model(X_test_t).numpy()\n",
    "    true = y_test_t.numpy()\n",
    "    pred_inv = scaler_temp.inverse_transform(pred)\n",
    "    true_inv = scaler_temp.inverse_transform(true)\n",
    "\n",
    "plt.plot(true_inv.flatten(), label='True')\n",
    "plt.plot(pred_inv.flatten(), label='Predicted')\n",
    "plt.title('NARX: Multi-step Temperature Forecast')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Temperature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90656f1",
   "metadata": {},
   "source": [
    "<a id=\"rl\"></a>\n",
    "# Reinforcement Learning\n",
    "\n",
    "This section demonstrates reinforcement learning algorithms using stable-baselines3 and OpenAI Gym environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effa26e",
   "metadata": {},
   "source": [
    "<a id=\"dqn\"></a>\n",
    "## Deep Q-Networks (DQN)\n",
    "\n",
    "We will use stable-baselines3 to train a DQN agent on the CartPole-v1 environment.\n",
    "\n",
    "**Reference:** [DQN Documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8579d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'shimmy>=2.0'\n",
    "!pip install --upgrade gymnasium stable-baselines3 shimmy\n",
    "!pip install numpy==1.23.5 --force-reinstall\n",
    "\n",
    "# Train DQN agent on CartPole-v1\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Create environment and train agent\n",
    "env = gym.make('CartPole-v1')\n",
    "dqn_model = DQN('MlpPolicy', env, verbose=1)\n",
    "dqn_model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate agent and collect rewards for plotting\n",
    "episode_rewards = []\n",
    "for _ in range(20):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = dqn_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "# Plot episode rewards\n",
    "plt.plot(range(1, len(episode_rewards) + 1), episode_rewards, marker='o')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN: CartPole-v1 Performance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d1736",
   "metadata": {},
   "source": [
    "<a id=\"pg\"></a>\n",
    "## Policy Gradient Methods (PPO)\n",
    "\n",
    "We will use stable-baselines3 to train a PPO agent on the LunarLander-v2 environment.\n",
    "\n",
    "**Reference:** [PPO Documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eff989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO agent on LunarLander-v2\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "ppo_model = PPO('MlpPolicy', env, verbose=1)\n",
    "ppo_model.learn(total_timesteps=20000)\n",
    "\n",
    "# Evaluate agent\n",
    "obs = env.reset()\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    rewards.append(total_reward)\n",
    "print(f\"Average reward over 10 episodes: {np.mean(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66611c7f",
   "metadata": {},
   "source": [
    "<a id=\"ac\"></a>\n",
    "## Actor-Critic Methods (A2C)\n",
    "\n",
    "We will use stable-baselines3 to train an A2C agent on the MountainCarContinuous-v0 environment.\n",
    "\n",
    "**Reference:** [A2C Documentation](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A2C agent on MountainCarContinuous-v0\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "a2c_model = A2C('MlpPolicy', env, verbose=1)\n",
    "a2c_model.learn(total_timesteps=20000)\n",
    "\n",
    "# Evaluate agent and plot rewards\n",
    "episode_rewards = []\n",
    "for _ in range(10):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = a2c_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "plt.bar(range(1, 11), episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('A2C: MountainCarContinuous-v0 Performance')\n",
    "plt.show()\n",
    "print(f\"Average reward over 10 episodes: {np.mean(episode_rewards):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
