{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "__Machine learning__ is a method of _data analysis_ that automates\n",
    "_analytical model building_.\n",
    "It is a branch of _artificial intelligence_ based on the idea that systems\n",
    "can _learn from data_, _identify patterns_ and _make decisions_ with\n",
    "minimal human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn (SKlearn, Scipy, Numpy)\n",
    "\n",
    "__Scikit-learn__ is a _Python package_ that provides a wide range of _machine learning algorithms_ and tools. \n",
    "It is built on top of _NumPy_, _SciPy_, and _Matplotlib_, and is designed to be simple and efficient for data analysis and modeling.\n",
    "\n",
    "__Scikit-learn__ offers various modules for tasks such as _classification_, _regression_, _clustering_, _dimensionality reduction_, and _model selection_.\n",
    "It also provides utilities for _preprocessing data_, _evaluating models_, and _handling datasets_.\n",
    "\n",
    "With its extensive documentation and user-friendly interface, __Scikit-learn__ is widely used in the field of machine learning and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spplitting the data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# importing the model\n",
    "football_df = pd.read_csv('csv-files/football_data.csv')\n",
    "\n",
    "# Assuming football_df is your DataFrame and it has a 'target' column\n",
    "# columns_list is a list of column names to be used as features\n",
    "columns_list = ['feature1', 'feature2', 'feature3']  # Replace with actual column names\n",
    "\n",
    "# Splitting the data into features and target\n",
    "X = football_df[columns_list]\n",
    "y = football_df['target']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "__K-Nearest Neighbors__ is a simple algorithm that _stores all available\n",
    "cases_ and _classifies_ new cases based on a similarity measure.\n",
    "\n",
    "It is a type of _instance-based learning_, or _lazy learning_, where the\n",
    "function is only approximated locally and all computation is deferred\n",
    "until function evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of the data\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Feature scaling for better performance of KNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creating the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
    "# Fitting the model with the training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Assuming new_input is a new read_csv\n",
    "# Predicting the class for the new input\n",
    "predicted_class = knn.predict(new_input_scaled)\n",
    "\n",
    "print(f\"The predicted class for the new input is: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression of the data\n",
    "# Feature scaling for better performance of KNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creating the KNN model for regression\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)  # You can adjust the number of neighbors\n",
    "\n",
    "# Fitting the model with the training data\n",
    "knn_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Assuming new_input is a new data point you want to predict\n",
    "# new_input should be a list of values corresponding to columns_list\n",
    "new_input_scaled = scaler.transform([new_input])  # Replace new_input with actual data\n",
    "predicted_target = knn_regressor.predict(new_input_scaled)\n",
    "\n",
    "print(predicted_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Least Squares\n",
    "\n",
    "__Linear regression__ is a type of _regression analysis_ used for predicting the value of a _continuous dependent variable_. It works by finding the _line that best fits the data_.\n",
    "\n",
    "_Least squares_ is a method for finding the _best-fitting_ line by __minimizing__ the _sum of the squared differences_ between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fitting the model with the training data\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Assuming new_input is a new data point you want to predict\n",
    "# new_input should be a list of values corresponding to columns_list\n",
    "new_input = [value1, value2, value3]  # Replace value1, value2, value3 with actual values\n",
    "\n",
    "# Predicting the target for the new input\n",
    "predicted_target = linear_regressor.predict([new_input])\n",
    "\n",
    "print(f\"The predicted target for the new input is: {predicted_target[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization with Ridge and Lasso\n",
    "\n",
    "__Ridge regression__ (_L2_) and __Lasso regression__ (_L1_) are a type of _linear regression_ that includes a _penalty_ term to __prevent overfitting__. They work by adding a _regularization term_ to the least squares objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing Rigde Regression (L2 regularization)\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Creating the Ridge Regression model\n",
    "# alpha is the regularization strength; larger values specify stronger regularization.\n",
    "ridge_regressor = Ridge(alpha=1.0)\n",
    "\n",
    "# Fitting the model with the training data\n",
    "ridge_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = ridge_regressor.predict(X_test)\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing Lasso Regression (L1 regularization)\n",
    "# Creating the Lasso Regression model\n",
    "# alpha is the regularization strength; larger values specify stronger regularization.\n",
    "lasso_regressor = Lasso(alpha=1.0)\n",
    "\n",
    "# Fitting the model with the training data\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = lasso_regressor.predict(X_test)\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# To understand feature sensitivity, you can look at the coefficients\n",
    "coefficients = pd.DataFrame(lasso_regressor.coef_, X.columns, columns=['Coefficient'])\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "__Polynomial regression__ is a type of r_egression analysis_ that models\n",
    "the _relationship_ between the independent and dependent variables as\n",
    "an $nth-degree$ _polynomial_. It can capture _non-linear relationships_ between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming football_df is your DataFrame and it has a 'target' column\n",
    "# columns_list is a list of column names to be used as features\n",
    "columns_list = ['feature1', 'feature2', 'feature3']  # Replace with actual column names\n",
    "\n",
    "# Splitting the data into features and target\n",
    "X = football_df[columns_list]\n",
    "y = football_df['target']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transforming the features into polynomial features\n",
    "degree = 2  # Degree of the polynomial features\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Creating the Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fitting the model with the polynomial features and the training data\n",
    "linear_regressor.fit(X_train_poly, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = linear_regressor.predict(X_test_poly)\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "__Logistic regression__ is a type of _regression analysis_ used for predicting the outcome of a _categorical dependent variable_.\n",
    "It is used for __binary classification__ tasks, where the output is a\n",
    "probability between $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Creating the Logistic Regression model\n",
    "logistic_regressor = LogisticRegression()\n",
    "\n",
    "# Fitting the model with the training data\n",
    "logistic_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = logistic_regressor.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "__Cross-validation__ is a technique for _assessing the performance_ of a\n",
    "model. It involves _splitting_ the data into multiple subsets, training the model on some subsets, and evaluating it on others.\n",
    "\n",
    "__Cross-validation__ helps to _reduce overfitting_ and provides a more\n",
    "accurate estimate of the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5)  # cv=5 for 5-fold cross-validation\n",
    "\n",
    "# Print the accuracy for each fold\n",
    "print(\"Accuracy for each fold: \", scores)\n",
    "\n",
    "# Print the mean accuracy of all 5 folds\n",
    "print(\"Mean cross-validation accuracy: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "__One-hot encoding__ is a technique for _converting_ _categorical_ variables into _numerical_ variables.\n",
    "\n",
    "It creates a _binary vector_ for each _category_, with a $1$ for the\n",
    "category and $0$s for all other categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Creating a fake DataFrame\n",
    "data = {\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red'],\n",
    "    'Size': ['S', 'M', 'L', 'XL', 'S'],\n",
    "    'Price': [10, 15, 20, 25, 10]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Applying OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(df[['Color', 'Size']]) # Encoding 'Color' and 'Size' columns\n",
    "encoded_data_dense = encoded_data.toarray()\n",
    "\n",
    "print(encoded_data_dense)\n",
    "print(encoder.get_feature_names_out(['Color', 'Size']))\n",
    "\n",
    "# Creating a DataFrame with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data_dense, columns=encoder.get_feature_names_out(['Color', 'Size']))\n",
    "\n",
    "# Concatenating the encoded columns with the original DataFrame (excluding the original 'Color' and 'Size' columns)\n",
    "final_df = pd.concat([df.drop(['Color', 'Size'], axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Display the final DataFrame after one-hot encoding\n",
    "print(\"\\nDataFrame after OneHotEncoding:\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "__Random forest__ is an _ensemble learning_ method that combines\n",
    "_multiple decision trees_ to create a strong predictive model.\n",
    "\n",
    "It works by building _multiple trees_ and averaging their predictions to\n",
    "_reduce overfitting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
      "0       GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
      "1       GP   F   17       U     GT3       T     1     1   at_home     other   \n",
      "2       GP   F   15       U     LE3       T     1     1   at_home     other   \n",
      "3       GP   F   15       U     GT3       T     4     2    health  services   \n",
      "4       GP   F   16       U     GT3       T     3     3     other     other   \n",
      "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
      "644     MS   F   19       R     GT3       T     2     3  services     other   \n",
      "645     MS   F   18       U     LE3       T     3     1   teacher  services   \n",
      "646     MS   F   18       U     GT3       T     1     1     other     other   \n",
      "647     MS   M   17       U     LE3       T     3     1  services  services   \n",
      "648     MS   M   18       R     LE3       T     3     2  services     other   \n",
      "\n",
      "     ... famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
      "0    ...      4        3      4     1     1      3        4   0  11  11  \n",
      "1    ...      5        3      3     1     1      3        2   9  11  11  \n",
      "2    ...      4        3      2     2     3      3        6  12  13  12  \n",
      "3    ...      3        2      2     1     1      5        0  14  14  14  \n",
      "4    ...      4        3      2     1     2      5        0  11  13  13  \n",
      "..   ...    ...      ...    ...   ...   ...    ...      ...  ..  ..  ..  \n",
      "644  ...      5        4      2     1     2      5        4  10  11  10  \n",
      "645  ...      4        3      4     1     1      1        4  15  15  16  \n",
      "646  ...      1        1      1     1     1      5        6  11  12   9  \n",
      "647  ...      2        4      5     3     4      2        6  10  10  10  \n",
      "648  ...      4        4      1     3     4      5        4  10  11  11  \n",
      "\n",
      "[649 rows x 33 columns]\n",
      "G3\n",
      "11    104\n",
      "10     97\n",
      "13     82\n",
      "12     72\n",
      "14     63\n",
      "15     49\n",
      "16     36\n",
      "8      35\n",
      "9      35\n",
      "17     29\n",
      "18     15\n",
      "0      15\n",
      "7      10\n",
      "6       3\n",
      "19      2\n",
      "1       1\n",
      "5       1\n",
      "Name: count, dtype: int64\n",
      "Accuracy: 0.41025641025641024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('student-por.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Assuming the target variable is 'Grade' and it's categorical\n",
    "le = LabelEncoder()\n",
    "\n",
    "print(df['G3'].value_counts())\n",
    "df['school'] = le.fit_transform(df['school'])\n",
    "df['sex'] = le.fit_transform(df['sex'])\n",
    "df['address'] = le.fit_transform(df['address'])\n",
    "df['famsize'] = le.fit_transform(df['famsize'])\n",
    "df['Pstatus'] = le.fit_transform(df['Pstatus'])\n",
    "df['Mjob'] = le.fit_transform(df['Mjob'])\n",
    "df['Fjob'] = le.fit_transform(df['Fjob'])\n",
    "df['reason'] = le.fit_transform(df['reason'])\n",
    "df['guardian'] = le.fit_transform(df['guardian'])\n",
    "\n",
    "X = df.drop(['G3', 'schoolsup', 'famsup', 'famrel', 'paid', 'romantic', 'activities', 'higher', 'internet', 'nursery'], axis=1)  # Features\n",
    "y = df['G3']  # Target\n",
    "\n",
    "# Normalize features\n",
    "#scaler = StandardScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "#print(y.value_counts())  # Display the distribution of the target variable\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Trees\n",
    "\n",
    "__Gradient boosted decision trees__ are an _ensemble learning_ method\n",
    "that combines _multiple decision trees_ and _gradient descedent\n",
    "optimization_ to create a strong predictive model.\n",
    "\n",
    "They work by building _trees sequentially_, with each tree _correcting the\n",
    "errors_ of the previous trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(eval_metric='mlogloss', objective='multi:softmax')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('datasets_kaggle/insurance.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Assuming 'charges' is the target variable\n",
    "X = df.drop('charges', axis=1)\n",
    "y = df['charges']\n",
    "\n",
    "# Encode categorical variables\n",
    "for column in X.select_dtypes(include=['object']).columns:\n",
    "    X[column] = LabelEncoder().fit_transform(X[column])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the XGBoost model\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 1.0,\n",
    "                max_depth = 20, alpha = 10, n_estimators = 20)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "__Neural networks__ are a type of _machine learning_ model inspired by\n",
    "the _human brain_.\n",
    "\n",
    "They consist of _layers of interconnected nodes_ that process input data\n",
    "and produce output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('datasets_kaggle/Student_performance_data _.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Assuming 'GradeClass' is the target variable and it's categorical\n",
    "X = df.drop('GradeClass', axis=1).values\n",
    "y = LabelEncoder().fit_transform(df['GradeClass'].values)\n",
    "y = to_categorical(y)  # Convert labels to one-hot encoding\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "A __confusion matrix__ is a table that _summarizes the performance_ of a\n",
    "classification model.\n",
    "\n",
    "It shows the number of _true positives_, _true negatives_, _false positives_,\n",
    "and _false negatives_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Metrics\n",
    "\n",
    "- __Accuracy:__ The proportion of correct predictions.\n",
    "- __Precision:__ The proportion of true positives among all positive\n",
    "predictions.\n",
    "- __Recall:__ The proportion of true positives among all actual positives.\n",
    "- __F1 Score:__ The harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming cm is the confusion matrix obtained from the Random Forest output\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_true and y_pred are defined\n",
    "# y_true = [...]  # True labels\n",
    "# y_pred = [...]  # Predictions made by the Random Forest model\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='binary')\n",
    "recall = recall_score(y_true, y_pred, average='binary')\n",
    "f1 = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Decision Metrics\n",
    "\n",
    "- __ROC Curve:__ A plot of the true positive rate against the false positive rate.\n",
    "- __Precision-Recall Curve:__ A plot of precision against recall.\n",
    "- __AUC-ROC:__ The area under the ROC curve.\n",
    "- __AUC-PR:__ The area under the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_true and y_pred_proba are defined\n",
    "# y_true = [...]  # True labels\n",
    "# y_pred_proba = [...]  # Predicted probabilities for the positive class\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_true, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision-Recall Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_true, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (area = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Evaluation Metrics\n",
    "\n",
    "- __Mean Squared Error:__ The average of the squared differences between the predicted and actual values.\n",
    "- __Mean Absolute Error:__ The average of the absolute differences between the predicted and actual values.\n",
    "- __R-Squared:__ The proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "- __Adjusted R-Squared:__ A modified version of R-squared that adjusts for the number of predictors in the model.\n",
    "- __Root Mean Squared Error:__ The square root of the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_true and y_pred are defined\n",
    "# y_true = [...]  # True values\n",
    "# y_pred = [...]  # Predictions made by the XGBoost regressor\n",
    "\n",
    "# Number of observations and predictors\n",
    "n = len(y_true)  # Number of observations\n",
    "p = model.n_features_in_\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r_squared = r2_score(y_true, y_pred)\n",
    "adjusted_r_squared = 1 - (1-r_squared) * (n-1) / (n-p-1)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"Adjusted R-squared: {adjusted_r_squared}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
