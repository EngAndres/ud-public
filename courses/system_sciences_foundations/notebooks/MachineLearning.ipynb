{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "__Machine learning__ is a method of _data analysis_ that automates\n",
    "_analytical model building_.\n",
    "It is a branch of _artificial intelligence_ based on the idea that systems\n",
    "can _learn from data_, _identify patterns_ and _make decisions_ with\n",
    "minimal human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn (SKlearn, Scipy, Numpy)\n",
    "\n",
    "__Scikit-learn__ is a _Python package_ that provides a wide range of _machine learning algorithms_ and tools. \n",
    "It is built on top of _NumPy_, _SciPy_, and _Matplotlib_, and is designed to be simple and efficient for data analysis and modeling.\n",
    "\n",
    "__Scikit-learn__ offers various modules for tasks such as _classification_, _regression_, _clustering_, _dimensionality reduction_, and _model selection_.\n",
    "It also provides utilities for _preprocessing data_, _evaluating models_, and _handling datasets_.\n",
    "\n",
    "With its extensive documentation and user-friendly interface, __Scikit-learn__ is widely used in the field of machine learning and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "__K-Nearest Neighbors__ is a simple algorithm that _stores all available\n",
    "cases_ and _classifies_ new cases based on a similarity measure.\n",
    "\n",
    "It is a type of _instance-based learning_, or _lazy learning_, where the\n",
    "function is only approximated locally and all computation is deferred\n",
    "until function evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8041958041958042\n",
      "The predicted class for the new input is: 0\n"
     ]
    }
   ],
   "source": [
    "# Classification of the data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "titanic_df = pd.read_csv('datasets/titanic.csv')\n",
    "titanic_df = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Survived']]\n",
    "titanic_df = titanic_df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = titanic_df.drop(['Survived'], axis=1)  \n",
    "y = titanic_df['Survived']\n",
    "\n",
    "# encode sex column using OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(X[['Sex']]) # Encoding 'Sex' column\n",
    "encoded_data_dense = encoded_data.toarray()\n",
    "encoded_df = pd.DataFrame(encoded_data_dense, columns=encoder.get_feature_names_out(['Sex']))\n",
    "X = pd.concat([X.drop('Sex', axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train_titanic, X_test_titanic, y_train_titanic, y_test_titanic = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling for better performance of KNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_titanic)\n",
    "X_test_scaled = scaler.transform(X_test_titanic)\n",
    "\n",
    "# Creating the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=7)  # You can adjust the number of neighbors\n",
    "\n",
    "# Fitting the model with the training data\n",
    "knn.fit(X_train_scaled, y_train_titanic)\n",
    "\n",
    "y_pred_titanic = knn.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test_titanic, y_pred_titanic)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Assuming new_input is a new row of the csv\n",
    "# Predicting the class for the new input\n",
    "new_input = pd.DataFrame([[3, 22, 0, 0, 0, 1]], \n",
    "                         columns=['Pclass', 'Age', 'SibSp', 'Parch', 'Sex_female', 'Sex_male'], \n",
    "                         index=[0])\n",
    "new_input_scaled = scaler.transform(new_input)\n",
    "predicted_class = knn.predict(new_input_scaled)\n",
    "\n",
    "print(f\"The predicted class for the new input is: {predicted_class[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/nylkt2zx4vx8clcb_x8whb100000gn/T/ipykernel_45764/2902111001.py:6: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  household_df = pd.read_csv('datasets/household_power_consumption.csv', sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.10335349953197798\n"
     ]
    }
   ],
   "source": [
    "# Regression of the data\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# load data and adjust datetime column\n",
    "household_df = pd.read_csv('datasets/household_power_consumption.csv', sep=';')\n",
    "# Clean data\n",
    "household_df = household_df.replace('?', np.nan)\n",
    "household_df = household_df.dropna().reset_index(drop=True)\n",
    "# Make just one datatime column\n",
    "household_df['Datetime'] = pd.to_datetime(household_df['Date'] + ' ' + household_df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "household_df = household_df.drop(['Date', 'Time'], axis=1)\n",
    "# Sort by datetime\n",
    "household_df = household_df.sort_values(by='Datetime')\n",
    "household_df = household_df.reset_index(drop=True)\n",
    "# convert datetime to unix timestamp\n",
    "household_df['Datetime'] = household_df['Datetime'].astype(np.int64) // 10**9  \n",
    "\n",
    "# split datasets\n",
    "X = household_df.drop(['Global_active_power'], axis=1)  # Features\n",
    "y = household_df['Global_active_power'].astype(float)  # Target variable\n",
    "# Split dataset into training and testing sets based on datetime\n",
    "train_percentage = 0.8\n",
    "X_train = X.iloc[:int(len(X) * train_percentage)]\n",
    "y_train = y.iloc[:int(len(y) * train_percentage)]\n",
    "X_test = X.iloc[int(len(X) * train_percentage):]\n",
    "y_test = y.iloc[int(len(y) * train_percentage):]\n",
    "\n",
    "# Feature scaling for better performance of KNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creating the KNN model for regression\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=7)  # You can adjust the number of neighbors\n",
    "\n",
    "# Fitting the model with the training data\n",
    "knn_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Validate\n",
    "y_pred = knn_regressor.predict(X_test_scaled)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Least Squares\n",
    "\n",
    "__Linear regression__ is a type of _regression analysis_ used for predicting the value of a _continuous dependent variable_. It works by finding the _line that best fits the data_.\n",
    "\n",
    "_Least squares_ is a method for finding the _best-fitting_ line by __minimizing__ the _sum of the squared differences_ between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Root Mean Squared Error: 0.03977637194254275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creating the Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fitting the model with the training data\n",
    "linear_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test the model \n",
    "y_pred_linear = linear_regressor.predict(X_test_scaled)\n",
    "rmse_linear = root_mean_squared_error(y_test, y_pred_linear)\n",
    "print(f\"Linear Regression Root Mean Squared Error: {rmse_linear}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization with Ridge and Lasso\n",
    "\n",
    "__Ridge regression__ (_L2_) and __Lasso regression__ (_L1_) are a type of _linear regression_ that includes a _penalty_ term to __prevent overfitting__. They work by adding a _regularization term_ to the least squares objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.03977639096485136\n"
     ]
    }
   ],
   "source": [
    "# implementing Rigde Regression (L2 regularization)\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Creating the Ridge Regression model\n",
    "# alpha is the regularization strength; larger values specify stronger regularization.\n",
    "ridge_regressor = Ridge(alpha=1.0)\n",
    "\n",
    "# Fitting the model with the training data\n",
    "ridge_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = ridge_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n",
    "mse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.8253175294821335\n",
      "                       Coefficient\n",
      "Global_reactive_power     0.000000\n",
      "Voltage                  -0.000000\n",
      "Global_intensity          0.091952\n",
      "Sub_metering_1            0.000000\n",
      "Sub_metering_2            0.000000\n",
      "Sub_metering_3            0.000000\n",
      "Datetime                 -0.000000\n"
     ]
    }
   ],
   "source": [
    "# implementing Lasso Regression (L1 regularization)\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Creating the Lasso Regression model\n",
    "# alpha is the regularization strength; larger values specify stronger regularization.\n",
    "lasso_regressor = Lasso(alpha=1.0)\n",
    "\n",
    "# Fitting the model with the training data\n",
    "lasso_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = lasso_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculating the root mean squared error of the predictions\n",
    "mse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# To understand feature sensitivity, you can look at the coefficients\n",
    "coefficients = pd.DataFrame(lasso_regressor.coef_, X.columns, columns=['Coefficient'])\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "__Polynomial regression__ is a type of r_egression analysis_ that models\n",
    "the _relationship_ between the independent and dependent variables as\n",
    "an $nth-degree$ _polynomial_. It can capture _non-linear relationships_ between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1639424, 571]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m linear_regressor = LinearRegression()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Fitting the model with the polynomial features and the training data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mlinear_regressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_poly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_titanic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Making predictions on the test set\u001b[39;00m\n\u001b[32m     18\u001b[39m y_pred = linear_regressor.predict(X_test_poly)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ud/lib/python3.13/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ud/lib/python3.13/site-packages/sklearn/linear_model/_base.py:618\u001b[39m, in \u001b[36mLinearRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    614\u001b[39m n_jobs_ = \u001b[38;5;28mself\u001b[39m.n_jobs\n\u001b[32m    616\u001b[39m accept_sparse = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.positive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcoo\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m has_sw = sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ud/lib/python3.13/site-packages/sklearn/utils/validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ud/lib/python3.13/site-packages/sklearn/utils/validation.py:1387\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1368\u001b[39m X = check_array(\n\u001b[32m   1369\u001b[39m     X,\n\u001b[32m   1370\u001b[39m     accept_sparse=accept_sparse,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1382\u001b[39m     input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1383\u001b[39m )\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m-> \u001b[39m\u001b[32m1387\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ud/lib/python3.13/site-packages/sklearn/utils/validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [1639424, 571]"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Transforming the features into polynomial features\n",
    "degree = 3  # Degree of the polynomial features\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly_features.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly_features.transform(X_test_scaled)\n",
    "\n",
    "# Creating the Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fitting the model with the polynomial features and the training data\n",
    "linear_regressor.fit(X_train_poly, y_train_titanic)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = linear_regressor.predict(X_test_poly)\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n",
    "mse = root_mean_squared_error(y_test_titanic, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "__Logistic regression__ is a type of _regression analysis_ used for predicting the outcome of a _categorical dependent variable_.\n",
    "It is used for __binary classification__ tasks, where the output is a\n",
    "probability between $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7552447552447552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_titanic)\n",
    "X_test_scaled = scaler.transform(X_test_titanic)\n",
    "\n",
    "# Creating the Logistic Regression model\n",
    "logistic_regressor = LogisticRegression()\n",
    "\n",
    "# Fitting the model with the training data\n",
    "logistic_regressor.fit(X_train_scaled, y_train_titanic)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = logistic_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculating the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test_titanic, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "__Cross-validation__ is a technique for _assessing the performance_ of a\n",
    "model. It involves _splitting_ the data into multiple subsets, training the model on some subsets, and evaluating it on others.\n",
    "\n",
    "__Cross-validation__ helps to _reduce overfitting_ and provides a more\n",
    "accurate estimate of the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold:  [0.79130435 0.76315789 0.8245614  0.88596491 0.8245614 ]\n",
      "Mean cross-validation accuracy:  0.8179099923722349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X_train_titanic, y_train_titanic, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the accuracy for each fold\n",
    "print(\"Accuracy for each fold: \", scores)\n",
    "\n",
    "# Print the mean accuracy of all 5 folds\n",
    "mean_accuracy = scores.mean()\n",
    "print(\"Mean cross-validation accuracy: \", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "__One-hot encoding__ is a technique for _converting_ _categorical_ variables into _numerical_ variables.\n",
    "\n",
    "It creates a _binary vector_ for each _category_, with a $1$ for the\n",
    "category and $0$s for all other categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Color Size  Price\n",
      "0    Red    S     10\n",
      "1   Blue    M     15\n",
      "2  Green    L     20\n",
      "3   Blue   XL     25\n",
      "4    Red    S     10\n",
      "\n",
      "DataFrame after OneHotEncoding:\n",
      "   Price  Color_Blue  Color_Green  Color_Red  Size_L  Size_M  Size_S  Size_XL\n",
      "0     10         0.0          0.0        1.0     0.0     0.0     1.0      0.0\n",
      "1     15         1.0          0.0        0.0     0.0     1.0     0.0      0.0\n",
      "2     20         0.0          1.0        0.0     1.0     0.0     0.0      0.0\n",
      "3     25         1.0          0.0        0.0     0.0     0.0     0.0      1.0\n",
      "4     10         0.0          0.0        1.0     0.0     0.0     1.0      0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Creating a fake DataFrame\n",
    "data = {\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red'],\n",
    "    'Size': ['S', 'M', 'L', 'XL', 'S'],\n",
    "    'Price': [10, 15, 20, 25, 10]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Applying OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(df[['Color', 'Size']]) # Encoding 'Color' and 'Size' columns\n",
    "encoded_data_dense = encoded_data.toarray()\n",
    "\n",
    "# Creating a DataFrame with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data_dense, columns=encoder.get_feature_names_out(['Color', 'Size']))\n",
    "\n",
    "# Concatenating the encoded columns with the original DataFrame (excluding the original 'Color' and 'Size' columns)\n",
    "final_df = pd.concat([df.drop(['Color', 'Size'], axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Display the final DataFrame after one-hot encoding\n",
    "print(\"\\nDataFrame after OneHotEncoding:\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "__Random forest__ is an _ensemble learning_ method that combines\n",
    "_multiple decision trees_ to create a strong predictive model.\n",
    "\n",
    "It works by building _multiple trees_ and averaging their predictions to\n",
    "_reduce overfitting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, column):\n",
    "    \"\"\"\n",
    "    One-hot encodes specified columns in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): Column name to be one-hot encoded.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame with one-hot encoded column.\n",
    "    \"\"\"\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded_data = encoder.fit_transform(df[[column]])  # Encoding specified column\n",
    "    encoded_data_dense = encoded_data.toarray()\n",
    "    encoded_df = pd.DataFrame(encoded_data_dense, columns=encoder.get_feature_names_out([column]))\n",
    "    df = pd.concat([df.drop(column, axis=1), encoded_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Root Mean Squared Error: 1.1677842116559312\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "students_df = pd.read_csv('datasets/student-performance.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Assuming the target variable is 'Grade' and it's categorical\n",
    "\n",
    "students_df = one_hot_encoder(students_df, 'school')\n",
    "students_df = one_hot_encoder(students_df, 'sex')\n",
    "students_df = one_hot_encoder(students_df, 'address')\n",
    "students_df = one_hot_encoder(students_df, 'famsize')\n",
    "students_df = one_hot_encoder(students_df, 'Pstatus')\n",
    "students_df = one_hot_encoder(students_df, 'Mjob')\n",
    "students_df = one_hot_encoder(students_df, 'Fjob')\n",
    "students_df = one_hot_encoder(students_df, 'reason')\n",
    "students_df = one_hot_encoder(students_df, 'guardian')\n",
    "students_df = one_hot_encoder(students_df, 'schoolsup')\n",
    "students_df = one_hot_encoder(students_df, 'famsup')\n",
    "students_df = one_hot_encoder(students_df, 'paid')\n",
    "students_df = one_hot_encoder(students_df, 'activities')\n",
    "students_df = one_hot_encoder(students_df, 'nursery')\n",
    "students_df = one_hot_encoder(students_df, 'higher')\n",
    "students_df = one_hot_encoder(students_df, 'internet')\n",
    "students_df = one_hot_encoder(students_df, 'romantic')\n",
    "\n",
    "X = students_df.drop(['G3'], axis=1)  # Features\n",
    "y = students_df['G3']  # Target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)   \n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestRegressor()\n",
    "# Add hyperparameters optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "clf = grid_search.best_estimator_\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "root_mean_squared_error = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error: {root_mean_squared_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Trees\n",
    "\n",
    "__Gradient boosted decision trees__ are an _ensemble learning_ method\n",
    "that combines _multiple decision trees_ and _gradient descedent\n",
    "optimization_ to create a strong predictive model.\n",
    "\n",
    "They work by building _trees sequentially_, with each tree _correcting the\n",
    "errors_ of the previous trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(571, 6) (571,)\n",
      "(143, 6) (143,)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_titanic)\n",
    "X_test_scaled = scaler.transform(X_test_titanic)\n",
    "\n",
    "print(X_train_scaled.shape, y_train_titanic.shape)\n",
    "print(X_test_scaled.shape, y_test_titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/casierrav/.pyenv/versions/ud/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: '@rpath/libomp.dylib'\\n  Referenced from: '/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib'\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#!pip install xgboost\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m      5\u001b[39m scaler = StandardScaler()\n\u001b[32m      6\u001b[39m X_train_scaled = scaler.fit_transform(X_train_titanic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ud/lib/python3.13/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ud/lib/python3.13/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ud/lib/python3.13/site-packages/xgboost/core.py:295\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ud/lib/python3.13/site-packages/xgboost/core.py:257\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    256\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    258\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    266\u001b[39m \n\u001b[32m    267\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    271\u001b[39m         )\n\u001b[32m    272\u001b[39m     _register_log_callback(lib)\n\u001b[32m    274\u001b[39m     libver = _lib_version(lib)\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/casierrav/.pyenv/versions/ud/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: '@rpath/libomp.dylib'\\n  Referenced from: '/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib'\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_titanic)\n",
    "X_test_scaled = scaler.transform(X_test_titanic)\n",
    "\n",
    "print(X_train_scaled.shape, y_train_titanic.shape)\n",
    "\n",
    "model = xgb.XGBClassifier(eval_metric='mlogloss', objective='multi:softmax')\n",
    "model.fit(X_train_scaled, y_train_titanic)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_titanic, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('datasets_kaggle/insurance.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Assuming 'charges' is the target variable\n",
    "X = df.drop('charges', axis=1)\n",
    "y = df['charges']\n",
    "\n",
    "# Encode categorical variables\n",
    "for column in X.select_dtypes(include=['object']).columns:\n",
    "    X[column] = LabelEncoder().fit_transform(X[column])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the XGBoost model\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 1.0,\n",
    "                max_depth = 20, alpha = 10, n_estimators = 20)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "__Neural networks__ are a type of _machine learning_ model inspired by\n",
    "the _human brain_.\n",
    "\n",
    "They consist of _layers of interconnected nodes_ that process input data\n",
    "and produce output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting keras\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting absl-py (from keras)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: numpy in /Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages (from keras) (2.2.6)\n",
      "Collecting rich (from keras)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading h5py-3.14.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.16.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (30 kB)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.5.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging in /Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages (from keras) (25.0)\n",
      "Collecting typing-extensions>=4.6.0 (from optree->keras)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages (from rich->keras) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m100.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading h5py-3.14.0-cp313-cp313-macosx_10_13_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m170.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp313-cp313-macosx_10_13_universal2.whl (667 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m667.7/667.7 kB\u001b[0m \u001b[31m548.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp313-cp313-macosx_10_13_universal2.whl (643 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m643.0/643.0 kB\u001b[0m \u001b[31m722.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, typing-extensions, ml-dtypes, mdurl, h5py, absl-py, optree, markdown-it-py, rich, keras\n",
      "Successfully installed absl-py-2.3.0 h5py-3.14.0 keras-3.10.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 optree-0.16.0 rich-14.0.0 typing-extensions-4.14.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "#df = pd.read_csv('datasets_kaggle/Student_performance_data _.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Assuming 'GradeClass' is the target variable and it's categorical\n",
    "#X = df.drop('GradeClass', axis=1).values\n",
    "#y = LabelEncoder().fit_transform(df['GradeClass'].values)\n",
    "#y = to_categorical(y)  # Convert labels to one-hot encoding\n",
    "\n",
    "# Normalize features\n",
    "#scaler = StandardScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "root_mean_squared_error = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error: {root_mean_squared_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "A __confusion matrix__ is a table that _summarizes the performance_ of a\n",
    "classification model.\n",
    "\n",
    "It shows the number of _true positives_, _true negatives_, _false positives_,\n",
    "and _false negatives_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[75 12]\n",
      " [16 40]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test_titanic, y_pred_titanic)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Metrics\n",
    "\n",
    "- __Accuracy:__ The proportion of correct predictions.\n",
    "- __Precision:__ The proportion of true positives among all positive\n",
    "predictions.\n",
    "- __Recall:__ The proportion of true positives among all actual positives.\n",
    "- __F1 Score:__ The harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8041958041958042\n",
      "Precision: 0.7692307692307693\n",
      "Recall: 0.7142857142857143\n",
      "F1 Score: 0.7407407407407408\n"
     ]
    }
   ],
   "source": [
    "# Assuming cm is the confusion matrix obtained from the Random Forest output\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8041958041958042\n",
      "Precision: 0.7692307692307693\n",
      "Recall: 0.7142857142857143\n",
      "F1 Score: 0.7407407407407407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_true and y_pred are defined\n",
    "# y_true = [...]  # True labels\n",
    "# y_pred = [...]  # Predictions made by the Random Forest model\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_titanic, y_pred_titanic)\n",
    "precision = precision_score(y_test_titanic, y_pred_titanic, average='binary')\n",
    "recall = recall_score(y_test_titanic, y_pred_titanic, average='binary')\n",
    "f1 = f1_score(y_test_titanic, y_pred_titanic, average='binary')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Decision Metrics\n",
    "\n",
    "- __ROC Curve:__ A plot of the true positive rate against the false positive rate.\n",
    "- __Precision-Recall Curve:__ A plot of precision against recall.\n",
    "- __AUC-ROC:__ The area under the ROC curve.\n",
    "- __AUC-PR:__ The area under the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp313-cp313-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp313-cp313-macosx_10_13_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.4-cp313-cp313-macosx_10_13_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-macosx_10_13_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp313-cp313-macosx_10_13_x86_64.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m40.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:02\u001b[0m00:09\u001b[0mm\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp313-cp313-macosx_10_13_x86_64.whl (271 kB)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/ud/bin/pip\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/pip/_internal/cli/main.py\"\u001b[0m, line \u001b[35m80\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    return \u001b[31mcommand.main\u001b[0m\u001b[1;31m(cmd_args)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\"\u001b[0m, line \u001b[35m157\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    with \u001b[31mself.main_context\u001b[0m\u001b[1;31m()\u001b[0m:\n",
      "         \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m148\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    \u001b[31mnext\u001b[0m\u001b[1;31m(self.gen)\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/pip/_internal/cli/command_context.py\"\u001b[0m, line \u001b[35m19\u001b[0m, in \u001b[35mmain_context\u001b[0m\n",
      "    with \u001b[1;31mself._main_context\u001b[0m:\n",
      "         \u001b[1;31m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m619\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    raise exc\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m604\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    if \u001b[31mcb\u001b[0m\u001b[1;31m(*exc_details)\u001b[0m:\n",
      "       \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m148\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    \u001b[31mnext\u001b[0m\u001b[1;31m(self.gen)\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m42\u001b[0m, in \u001b[35mglobal_tempdir_manager\u001b[0m\n",
      "    with \u001b[31mExitStack\u001b[0m\u001b[1;31m()\u001b[0m as stack:\n",
      "         \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m619\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    raise exc\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m604\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    if \u001b[31mcb\u001b[0m\u001b[1;31m(*exc_details)\u001b[0m:\n",
      "       \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m169\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    \u001b[31mself.cleanup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m212\u001b[0m, in \u001b[35mcleanup\u001b[0m\n",
      "    \u001b[31mrmtree\u001b[0m\u001b[1;31m(self._path, ignore_errors=False)\u001b[0m\n",
      "    \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/pip/_internal/utils/retry.py\"\u001b[0m, line \u001b[35m34\u001b[0m, in \u001b[35mretry_wrapped\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/pip/_internal/utils/misc.py\"\u001b[0m, line \u001b[35m136\u001b[0m, in \u001b[35mrmtree\u001b[0m\n",
      "    \u001b[31mshutil.rmtree\u001b[0m\u001b[1;31m(dir, onexc=handler)\u001b[0m  # type: ignore\n",
      "    \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/lib/python3.13/shutil.py\"\u001b[0m, line \u001b[35m763\u001b[0m, in \u001b[35mrmtree\u001b[0m\n",
      "    \u001b[31m_rmtree_safe_fd\u001b[0m\u001b[1;31m(stack, onexc)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/casierrav/.pyenv/versions/3.13.3/lib/python3.13/shutil.py\"\u001b[0m, line \u001b[35m696\u001b[0m, in \u001b[35m_rmtree_safe_fd\u001b[0m\n",
      "    \u001b[31mos.unlink\u001b[0m\u001b[1;31m(entry.name, dir_fd=topfd)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install matplotlib\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve, precision_recall_curve, auc\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Assuming y_true and y_pred_proba are defined\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# y_true = [...]  # True labels\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# y_pred_proba = [...]  # Predicted probabilities for the positive class\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Calculate ROC Curve\u001b[39;00m\n\u001b[32m     10\u001b[39m fpr, tpr, thresholds_roc = roc_curve(y_test_titanic, y_pred_titanic)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_true and y_pred_proba are defined\n",
    "# y_true = [...]  # True labels\n",
    "# y_pred_proba = [...]  # Predicted probabilities for the positive class\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test_titanic, y_pred_titanic)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision-Recall Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test_titanic, y_pred_titanic)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (area = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Evaluation Metrics\n",
    "\n",
    "- __Mean Squared Error:__ The average of the squared differences between the predicted and actual values.\n",
    "- __Mean Absolute Error:__ The average of the absolute differences between the predicted and actual values.\n",
    "- __R-Squared:__ The proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "- __Adjusted R-Squared:__ A modified version of R-squared that adjusts for the number of predictors in the model.\n",
    "- __Root Mean Squared Error:__ The square root of the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_true and y_pred are defined\n",
    "# y_true = [...]  # True values\n",
    "# y_pred = [...]  # Predictions made by the XGBoost regressor\n",
    "\n",
    "# Number of observations and predictors\n",
    "n = len(y_true)  # Number of observations\n",
    "p = model.n_features_in_\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r_squared = r2_score(y_true, y_pred)\n",
    "adjusted_r_squared = 1 - (1-r_squared) * (n-1) / (n-p-1)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"Adjusted R-squared: {adjusted_r_squared}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
