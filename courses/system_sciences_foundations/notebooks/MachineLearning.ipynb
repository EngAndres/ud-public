{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "__Machine learning__ is a method of _data analysis_ that automates\n",
    "_analytical model building_.\n",
    "It is a branch of _artificial intelligence_ based on the idea that systems\n",
    "can _learn from data_, _identify patterns_ and _make decisions_ with\n",
    "minimal human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn (SKlearn, Scipy, Numpy)\n",
    "\n",
    "__Scikit-learn__ is a _Python package_ that provides a wide range of _machine learning algorithms_ and tools. \n",
    "It is built on top of _NumPy_, _SciPy_, and _Matplotlib_, and is designed to be simple and efficient for data analysis and modeling.\n",
    "\n",
    "__Scikit-learn__ offers various modules for tasks such as _classification_, _regression_, _clustering_, _dimensionality reduction_, and _model selection_.\n",
    "It also provides utilities for _preprocessing data_, _evaluating models_, and _handling datasets_.\n",
    "\n",
    "With its extensive documentation and user-friendly interface, __Scikit-learn__ is widely used in the field of machine learning and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "__K-Nearest Neighbors__ is a simple algorithm that _stores all available\n",
    "cases_ and _classifies_ new cases based on a similarity measure.\n",
    "\n",
    "It is a type of _instance-based learning_, or _lazy learning_, where the\n",
    "function is only approximated locally and all computation is deferred\n",
    "until function evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8041958041958042\n",
      "The predicted class for the new input is: 0\n"
     ]
    }
   ],
   "source": [
    "# Classification of the data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "titanic_df = pd.read_csv('datasets/titanic.csv')\n",
    "titanic_df = titanic_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Survived']]\n",
    "titanic_df = titanic_df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = titanic_df.drop(['Survived'], axis=1)  \n",
    "y = titanic_df['Survived']\n",
    "\n",
    "# encode sex column using OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(X[['Sex']]) # Encoding 'Sex' column\n",
    "encoded_data_dense = encoded_data.toarray()\n",
    "encoded_df = pd.DataFrame(encoded_data_dense, columns=encoder.get_feature_names_out(['Sex']))\n",
    "X = pd.concat([X.drop('Sex', axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train_titanic, X_test_titanic, y_train_titanic, y_test_titanic = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling for better performance of KNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_titanic)\n",
    "X_test_scaled = scaler.transform(X_test_titanic)\n",
    "\n",
    "# Creating the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=7)  # You can adjust the number of neighbors\n",
    "\n",
    "# Fitting the model with the training data\n",
    "knn.fit(X_train_scaled, y_train_titanic)\n",
    "\n",
    "y_pred_titanic = knn.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test_titanic, y_pred_titanic)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Assuming new_input is a new row of the csv\n",
    "# Predicting the class for the new input\n",
    "new_input = pd.DataFrame([[3, 22, 0, 0, 0, 1]], \n",
    "                         columns=['Pclass', 'Age', 'SibSp', 'Parch', 'Sex_female', 'Sex_male'], \n",
    "                         index=[0])\n",
    "new_input_scaled = scaler.transform(new_input)\n",
    "predicted_class = knn.predict(new_input_scaled)\n",
    "\n",
    "print(f\"The predicted class for the new input is: {predicted_class[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/nylkt2zx4vx8clcb_x8whb100000gn/T/ipykernel_6054/2902111001.py:6: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  household_df = pd.read_csv('datasets/household_power_consumption.csv', sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.10335349953197798\n"
     ]
    }
   ],
   "source": [
    "# Regression of the data\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# load data and adjust datetime column\n",
    "household_df = pd.read_csv('datasets/household_power_consumption.csv', sep=';')\n",
    "# Clean data\n",
    "household_df = household_df.replace('?', np.nan)\n",
    "household_df = household_df.dropna().reset_index(drop=True)\n",
    "# Make just one datatime column\n",
    "household_df['Datetime'] = pd.to_datetime(household_df['Date'] + ' ' + household_df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "household_df = household_df.drop(['Date', 'Time'], axis=1)\n",
    "# Sort by datetime\n",
    "household_df = household_df.sort_values(by='Datetime')\n",
    "household_df = household_df.reset_index(drop=True)\n",
    "# convert datetime to unix timestamp\n",
    "household_df['Datetime'] = household_df['Datetime'].astype(np.int64) // 10**9  \n",
    "\n",
    "# split datasets\n",
    "X = household_df.drop(['Global_active_power'], axis=1)  # Features\n",
    "y = household_df['Global_active_power'].astype(float)  # Target variable\n",
    "# Split dataset into training and testing sets based on datetime\n",
    "train_percentage = 0.8\n",
    "X_train = X.iloc[:int(len(X) * train_percentage)]\n",
    "y_train = y.iloc[:int(len(y) * train_percentage)]\n",
    "X_test = X.iloc[int(len(X) * train_percentage):]\n",
    "y_test = y.iloc[int(len(y) * train_percentage):]\n",
    "\n",
    "# Feature scaling for better performance of KNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creating the KNN model for regression\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=7)  # You can adjust the number of neighbors\n",
    "\n",
    "# Fitting the model with the training data\n",
    "knn_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Validate\n",
    "y_pred = knn_regressor.predict(X_test_scaled)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Least Squares\n",
    "\n",
    "__Linear regression__ is a type of _regression analysis_ used for predicting the value of a _continuous dependent variable_. It works by finding the _line that best fits the data_.\n",
    "\n",
    "_Least squares_ is a method for finding the _best-fitting_ line by __minimizing__ the _sum of the squared differences_ between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Root Mean Squared Error: 0.03977637194254275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creating the Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fitting the model with the training data\n",
    "linear_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test the model \n",
    "y_pred_linear = linear_regressor.predict(X_test_scaled)\n",
    "rmse_linear = root_mean_squared_error(y_test, y_pred_linear)\n",
    "print(f\"Linear Regression Root Mean Squared Error: {rmse_linear}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization with Ridge and Lasso\n",
    "\n",
    "__Ridge regression__ (_L2_) and __Lasso regression__ (_L1_) are a type of _linear regression_ that includes a _penalty_ term to __prevent overfitting__. They work by adding a _regularization term_ to the least squares objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.001582161278188709\n"
     ]
    }
   ],
   "source": [
    "# implementing Rigde Regression (L2 regularization)\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Creating the Ridge Regression model\n",
    "# alpha is the regularization strength; larger values specify stronger regularization.\n",
    "ridge_regressor = Ridge(alpha=1.0)\n",
    "\n",
    "# Fitting the model with the training data\n",
    "ridge_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = ridge_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.03128976705026332\n",
      "                        Coefficient\n",
      "Global_reactive_power  0.000000e+00\n",
      "Voltage               -0.000000e+00\n",
      "Global_intensity       1.838122e-01\n",
      "Sub_metering_1         0.000000e+00\n",
      "Sub_metering_2         0.000000e+00\n",
      "Sub_metering_3         5.686804e-03\n",
      "Datetime              -1.533729e-10\n"
     ]
    }
   ],
   "source": [
    "# implementing Lasso Regression (L1 regularization)\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Creating the Lasso Regression model\n",
    "# alpha is the regularization strength; larger values specify stronger regularization.\n",
    "lasso_regressor = Lasso(alpha=1.0)\n",
    "\n",
    "# Fitting the model with the training data\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = lasso_regressor.predict(X_test)\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# To understand feature sensitivity, you can look at the coefficients\n",
    "coefficients = pd.DataFrame(lasso_regressor.coef_, X.columns, columns=['Coefficient'])\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "__Polynomial regression__ is a type of r_egression analysis_ that models\n",
    "the _relationship_ between the independent and dependent variables as\n",
    "an $nth-degree$ _polynomial_. It can capture _non-linear relationships_ between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0015314718393658984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Transforming the features into polynomial features\n",
    "degree = 3  # Degree of the polynomial features\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly_features.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly_features.transform(X_test_scaled)\n",
    "\n",
    "# Creating the Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fitting the model with the polynomial features and the training data\n",
    "linear_regressor.fit(X_train_poly, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = linear_regressor.predict(X_test_poly)\n",
    "\n",
    "# Calculating the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "__Logistic regression__ is a type of _regression analysis_ used for predicting the outcome of a _categorical dependent variable_.\n",
    "It is used for __binary classification__ tasks, where the output is a\n",
    "probability between $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7552447552447552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Creating the Logistic Regression model\n",
    "logistic_regressor = LogisticRegression()\n",
    "\n",
    "# Fitting the model with the training data\n",
    "logistic_regressor.fit(X_train_titanic, y_train_titanic)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = logistic_regressor.predict(X_test_titanic)\n",
    "\n",
    "# Calculating the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test_titanic, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "__Cross-validation__ is a technique for _assessing the performance_ of a\n",
    "model. It involves _splitting_ the data into multiple subsets, training the model on some subsets, and evaluating it on others.\n",
    "\n",
    "__Cross-validation__ helps to _reduce overfitting_ and provides a more\n",
    "accurate estimate of the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold:  [0.79130435 0.76315789 0.8245614  0.88596491 0.8245614 ]\n",
      "Mean cross-validation accuracy:  0.8179099923722349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X_train_titanic, y_train_titanic, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the accuracy for each fold\n",
    "print(\"Accuracy for each fold: \", scores)\n",
    "\n",
    "# Print the mean accuracy of all 5 folds\n",
    "mean_accuracy = scores.mean()\n",
    "print(\"Mean cross-validation accuracy: \", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "__One-hot encoding__ is a technique for _converting_ _categorical_ variables into _numerical_ variables.\n",
    "\n",
    "It creates a _binary vector_ for each _category_, with a $1$ for the\n",
    "category and $0$s for all other categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Color Size  Price\n",
      "0    Red    S     10\n",
      "1   Blue    M     15\n",
      "2  Green    L     20\n",
      "3   Blue   XL     25\n",
      "4    Red    S     10\n",
      "\n",
      "DataFrame after OneHotEncoding:\n",
      "   Price  Color_Blue  Color_Green  Color_Red  Size_L  Size_M  Size_S  Size_XL\n",
      "0     10         0.0          0.0        1.0     0.0     0.0     1.0      0.0\n",
      "1     15         1.0          0.0        0.0     0.0     1.0     0.0      0.0\n",
      "2     20         0.0          1.0        0.0     1.0     0.0     0.0      0.0\n",
      "3     25         1.0          0.0        0.0     0.0     0.0     0.0      1.0\n",
      "4     10         0.0          0.0        1.0     0.0     0.0     1.0      0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Creating a fake DataFrame\n",
    "data = {\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red'],\n",
    "    'Size': ['S', 'M', 'L', 'XL', 'S'],\n",
    "    'Price': [10, 15, 20, 25, 10]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Applying OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(df[['Color', 'Size']]) # Encoding 'Color' and 'Size' columns\n",
    "encoded_data_dense = encoded_data.toarray()\n",
    "\n",
    "# Creating a DataFrame with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data_dense, columns=encoder.get_feature_names_out(['Color', 'Size']))\n",
    "\n",
    "# Concatenating the encoded columns with the original DataFrame (excluding the original 'Color' and 'Size' columns)\n",
    "final_df = pd.concat([df.drop(['Color', 'Size'], axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Display the final DataFrame after one-hot encoding\n",
    "print(\"\\nDataFrame after OneHotEncoding:\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "__Random forest__ is an _ensemble learning_ method that combines\n",
    "_multiple decision trees_ to create a strong predictive model.\n",
    "\n",
    "It works by building _multiple trees_ and averaging their predictions to\n",
    "_reduce overfitting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, column):\n",
    "    \"\"\"\n",
    "    One-hot encodes specified columns in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): Column name to be one-hot encoded.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame with one-hot encoded column.\n",
    "    \"\"\"\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded_data = encoder.fit_transform(df[[column]])  # Encoding specified column\n",
    "    encoded_data_dense = encoded_data.toarray()\n",
    "    encoded_df = pd.DataFrame(encoded_data_dense, columns=encoder.get_feature_names_out([column]))\n",
    "    df = pd.concat([df.drop(column, axis=1), encoded_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "students_df = pd.read_csv('datasets/student-performance.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Assuming the target variable is 'Grade' and it's categorical\n",
    "\n",
    "students_df = one_hot_encoder(students_df, 'school')\n",
    "students_df = one_hot_encoder(students_df, 'sex')\n",
    "students_df = one_hot_encoder(students_df, 'address')\n",
    "students_df = one_hot_encoder(students_df, 'famsize')\n",
    "students_df = one_hot_encoder(students_df, 'Pstatus')\n",
    "students_df = one_hot_encoder(students_df, 'Mjob')\n",
    "students_df = one_hot_encoder(students_df, 'Fjob')\n",
    "students_df = one_hot_encoder(students_df, 'reason')\n",
    "students_df = one_hot_encoder(students_df, 'guardian')\n",
    "students_df = one_hot_encoder(students_df, 'schoolsup')\n",
    "students_df = one_hot_encoder(students_df, 'famsup')\n",
    "students_df = one_hot_encoder(students_df, 'paid')\n",
    "students_df = one_hot_encoder(students_df, 'activities')\n",
    "students_df = one_hot_encoder(students_df, 'nursery')\n",
    "students_df = one_hot_encoder(students_df, 'higher')\n",
    "students_df = one_hot_encoder(students_df, 'internet')\n",
    "students_df = one_hot_encoder(students_df, 'romantic')\n",
    "\n",
    "X = students_df.drop(['G3'], axis=1)  # Features\n",
    "y = students_df['G3']  # Target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)   \n",
    "\n",
    "# Train the model\n",
    "clf = RandomForestRegressor()\n",
    "# Add hyperparameters optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "clf = grid_search.best_estimator_\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "root_mean_squared_error = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error: {root_mean_squared_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Trees\n",
    "\n",
    "__Gradient boosted decision trees__ are an _ensemble learning_ method\n",
    "that combines _multiple decision trees_ and _gradient descedent\n",
    "optimization_ to create a strong predictive model.\n",
    "\n",
    "They work by building _trees sequentially_, with each tree _correcting the\n",
    "errors_ of the previous trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-macosx_10_15_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: scipy in /Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages (from xgboost) (1.16.0)\n",
      "Downloading xgboost-3.0.2-py3-none-macosx_10_15_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: '@rpath/libomp.dylib'\\n  Referenced from: '/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib'\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install xgboost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m      5\u001b[39m model = xgb.XGBClassifier(eval_metric=\u001b[33m'\u001b[39m\u001b[33mmlogloss\u001b[39m\u001b[33m'\u001b[39m, objective=\u001b[33m'\u001b[39m\u001b[33mmulti:softmax\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m model.fit(X_train, y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/core.py:295\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/core.py:257\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    256\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    258\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    266\u001b[39m \n\u001b[32m    267\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    271\u001b[39m         )\n\u001b[32m    272\u001b[39m     _register_log_callback(lib)\n\u001b[32m    274\u001b[39m     libver = _lib_version(lib)\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: '@rpath/libomp.dylib'\\n  Referenced from: '/Users/casierrav/.pyenv/versions/3.13.3/envs/ud/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib'\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/Users/casierrav/.pyenv/versions/3.13.3/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(eval_metric='mlogloss', objective='multi:softmax')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('datasets_kaggle/insurance.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Assuming 'charges' is the target variable\n",
    "X = df.drop('charges', axis=1)\n",
    "y = df['charges']\n",
    "\n",
    "# Encode categorical variables\n",
    "for column in X.select_dtypes(include=['object']).columns:\n",
    "    X[column] = LabelEncoder().fit_transform(X[column])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the XGBoost model\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 1.0,\n",
    "                max_depth = 20, alpha = 10, n_estimators = 20)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "__Neural networks__ are a type of _machine learning_ model inspired by\n",
    "the _human brain_.\n",
    "\n",
    "They consist of _layers of interconnected nodes_ that process input data\n",
    "and produce output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('datasets_kaggle/Student_performance_data _.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Assuming 'GradeClass' is the target variable and it's categorical\n",
    "X = df.drop('GradeClass', axis=1).values\n",
    "y = LabelEncoder().fit_transform(df['GradeClass'].values)\n",
    "y = to_categorical(y)  # Convert labels to one-hot encoding\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "A __confusion matrix__ is a table that _summarizes the performance_ of a\n",
    "classification model.\n",
    "\n",
    "It shows the number of _true positives_, _true negatives_, _false positives_,\n",
    "and _false negatives_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Metrics\n",
    "\n",
    "- __Accuracy:__ The proportion of correct predictions.\n",
    "- __Precision:__ The proportion of true positives among all positive\n",
    "predictions.\n",
    "- __Recall:__ The proportion of true positives among all actual positives.\n",
    "- __F1 Score:__ The harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming cm is the confusion matrix obtained from the Random Forest output\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_true and y_pred are defined\n",
    "# y_true = [...]  # True labels\n",
    "# y_pred = [...]  # Predictions made by the Random Forest model\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='binary')\n",
    "recall = recall_score(y_true, y_pred, average='binary')\n",
    "f1 = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Decision Metrics\n",
    "\n",
    "- __ROC Curve:__ A plot of the true positive rate against the false positive rate.\n",
    "- __Precision-Recall Curve:__ A plot of precision against recall.\n",
    "- __AUC-ROC:__ The area under the ROC curve.\n",
    "- __AUC-PR:__ The area under the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_true and y_pred_proba are defined\n",
    "# y_true = [...]  # True labels\n",
    "# y_pred_proba = [...]  # Predicted probabilities for the positive class\n",
    "\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_true, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision-Recall Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_true, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (area = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Evaluation Metrics\n",
    "\n",
    "- __Mean Squared Error:__ The average of the squared differences between the predicted and actual values.\n",
    "- __Mean Absolute Error:__ The average of the absolute differences between the predicted and actual values.\n",
    "- __R-Squared:__ The proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "- __Adjusted R-Squared:__ A modified version of R-squared that adjusts for the number of predictors in the model.\n",
    "- __Root Mean Squared Error:__ The square root of the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_true and y_pred are defined\n",
    "# y_true = [...]  # True values\n",
    "# y_pred = [...]  # Predictions made by the XGBoost regressor\n",
    "\n",
    "# Number of observations and predictors\n",
    "n = len(y_true)  # Number of observations\n",
    "p = model.n_features_in_\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r_squared = r2_score(y_true, y_pred)\n",
    "adjusted_r_squared = 1 - (1-r_squared) * (n-1) / (n-p-1)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"Adjusted R-squared: {adjusted_r_squared}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
